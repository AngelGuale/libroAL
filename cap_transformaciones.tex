%%transformaciones
\chapter{Transformaciones Lineales}

\begin{dfn}
Sean $V$ y $W$ dos espacios vectoriales sobre un campo $\mathbb{K}$. Una función $T: V \to W$ se dice que es lineal si para todo $u$, $v$ en $V$ y todo $\lambda \in \mathbb{K}$, se cumple que
\begin{enumerate}
\item $T (u+v) = T(u) + T(v)$
\item $T(\lambda u) = \lambda T(u)$
\end{enumerate}
\end{dfn}

\begin{ejemplo}
Sea $\alpha \in \mathbb{K}$ y $V$ un espacio vectorial sobre un campo $\mathbb{K}$. La función 
\begin{align*}
T: V &\to V\\
T(v) &= \alpha v
\end{align*}
es una transformación lineal.\\
En efecto 
\begin{align*}
T(u+v) &= \alpha (u+v)\\
&= \alpha u + \alpha v\\
&= T(u)+T(v)
\end{align*}
Si $\lambda \in \mathbb{K}$
\begin{align*}
T(\lambda u) &= \alpha (\lambda u)\\
&= \lambda (\alpha u)\\
&=\lambda T(u)
\end{align*}
\end{ejemplo}

\begin{ejemplo}
Sea $T: \rdos \to \rdos$ definida por
$$T(x,y) = (x \cos(\theta) - y\, \mathrm{sen} (\theta) ,\, x\, \mathrm{sen}(\theta) + y \cos (\theta))$$
$T$ es una transformación lineal que consiste en una rotación, de $\theta$ grados, alrededor del origen.
\end{ejemplo}

\begin{ejemplo}
Sea $T: \mathbb{R} \to \mathbb{R}$ definida por 
$$T(x) = x^2$$
Claramente $T$ no es una transformación lineal pues
$$(x+y)^2 \neq x^2 + y^2  \ \ \ \textup{si} \; x\neq 0 ,\; y \neq 0$$
\end{ejemplo}

\begin{theorem}
Sean $U$ y $V$ dos espacios vectoriales y $T: U \to V$ una transformación lineal, entonces se cumple que
\begin{itemize}
\item[a. ] $T(\mathbf{0}_u) = \mathbf{0}_v$
\item[b. ] $T(-v) = -T(v)$
\item[c. ] $T(u - v) = T(u) - T(v)$
\item[d. ] Si $v = \alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_n v_n$, entonces $T(v) = \alpha_1 T(v_1) + \alpha_2 T(v_2) + \ldots + \alpha_n T(v_n)$
\end{itemize}
\end{theorem}

\begin{proof}
\begin{itemize}
\item[a. ] $T(\mathbf{0}_u) = T(\mathbf{0}_u + \mathbf{0}_u) = T(\mathbf{0}_u) + T(\mathbf{0}_u)$, entonces $\mathbf{0}_v = T(\mathbf{0}_u)- T(\mathbf{0}_u) = T(\mathbf{0}_u)$
\item[b. ] $T(-v) = T((-1)v) = (-1) T(v) = -T(v)$
\item [c. ] $T(u-v) = T(u+(-1)v) = T(u) + T((-1)v) = T(u) - T(v)$
\item [d. ] Aplicando $n$ veces la linealidad de $T$ se obtiene la igualdad deseada.
\end{itemize}
\end{proof}

\begin{theorem}
\label{th:ag}
Sean $V$ y $W$ dos espacios vectoriales y $T: V \to W$ una transformación lineal. Si $B = \conjvect{v}{n}$ es una base para $V$ entonces $T$ está unívocamente determinada por los vectores \llav{T(v_1), T(v_2), \ldots , T(v_n)}.
\end{theorem}

\begin{proof}
En efecto, probemos que si $L: V \to W$ es una transformación lineal tal que $L(v_i) = T(v_i)$ para $i = 1, 2, \ldots , n$ entonces $L = T$. Necesitamos probar que para todo $v \in V$, $L(v) = T(v)$.
\\

Como $B$ es una base para $V$, se tiene que, para cualquier $v \in V$, existen escalares $\alpha_1, \alpha_2, \ldots , \alpha_n$ tales que $v = \alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_n v_n$.\\

Ahora
\begin{align*}
L(v) &= L(\alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_n v_n)\\
&= L(\alpha_1 v_1) + L(\alpha_2 v_2) + \ldots + L(\alpha_n v_n)\\
&=\alpha_1 L(v_1) + \alpha_2 L(v_2) + \ldots + \alpha_n L(v_n)\\ 
&=\alpha_1 T(v_1) + \alpha_2 T(v_2) + \ldots + \alpha_n T(v_n)\\
&= T(\alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_n v_n)\\
&= T(v) 
\end{align*}
\end{proof}



\begin{theorem}
Sea $A$ una matriz $m \times n$ con entradas reales. Entonces la función $T : \rn \to \rmm $ definida por $$T(v)= A v$$
es una transformación lineal.
\end{theorem}

\begin{proof}
Sin pérdida de rigurosidad representamos a un elemento $u$ de \rn \ como un vector columna $n \times 1$ y a los elementos de \rmm \ como vectores columna $m \times 1$.
\end{proof}


\section{Núcleo de una Transformación lineal}
\begin{dfn}
Sean $V$ y $W$ dos espacios vectoriales y $T : V \to W$ una transformación lineal. El núcleo de $T$, se define como el conjunto dado por: 
$$Nu(T) = \{v \in V : T(v) = \mathbf{0}_W\}$$
\end{dfn}

\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales y $T : V \to W$ una transformación lineal. Entonces $Nu(T)$es un subespacio vectorial de $V$.
\end{theorem}

\begin{proof}
Veamos que $Nu(T)$ es un subespacio vectorial de $V$.
\begin{enumerate}[i.]
\item Como $T(\mathbf{0}_V)=\mathbf{0}_W$ , entonces $\mathbf{0}_V \in Nu(T)$.
\item Supongamos que $u , v \in Nu(T)$, entonces $T(u+v) = T(u) + T(v) = \mathbf{0}_W + \mathbf{0}_W = \mathbf{0}_W$. Por lo tanto $u+v \in Nu(T)$.
\item Si $\alpha \in \mathbb{K}$ y $u \in Nu(T)$, se tiene que $T(\alpha u) = \alpha T(u) = \alpha \mathbf{0}_W = \mathbf{0}_W$ lo cual prueba que $\alpha u \in Nu(T)$.
\end{enumerate}
Así tenemos que $Nu(T)$ es un subespacio vectorial de $V$.
\end{proof}

\begin{ejemplo}
Sea $T: \pdos \to \rtres$ una transformación lineal definida por:
$$T(ax^2 + bx + c) = \vectrtrescent{a+b}{2a+b-c}{a-c}$$
Determine $Nu(T)$
\end{ejemplo}

\begin{sol}
El $Nu(T)$ se define como el conjunto de todos los vectores de $\pdos$ que, al transformarlos, obtengo el vector neutro de $\rtres$, entonces:
$$T(ax^2 + bx + c) = \vectrtrescent{0}{0}{0}$$
Entonces
$$\vectrtrescent{a+b}{2a+b-c}{a-c} = \vectrtrescent{0}{0}{0}$$
Sabemos que dos vectores son iguales si y solo si sus componentes también lo son, entonces obtenemos el siguiente sistema de ecuaciones
$$\reducir{rrr|r}{
1&1&0&0\\
2&1&-1&0\\
1&0&-1&0
}$$
Resolviéndolo por el método de Gauss, obtenemos:
$$\reducir{rrr|r}{
1&1&0&0\\
2&1&-1&0\\
1&0&-1&0
}
\underrightarrow{\begin{array}{r}
    2f_1 - f_2\\
    f_1 - f_3
\end{array}}
\reducir{rrr|r}{
1&1&0&0\\
0&1&1&0\\
0&1&1&0
}
\underrightarrow{f_2 - f_3}
\reducir{rrr|r}{
1&1&0&0\\
0&1&1&0\\
0&0&0&0
}$$
Por lo tanto, el sistema que hace que la transformación de un vector $ax^2 + bx +c$ sea igual a $\vectrtrescent{0}{0}{0}$ es el siguiente:
$$\svpdos{Nu(T)}{a}{b}{c}{\begin{array}{r}
        a + b = 0\\
        b + c = 0
    \end{array}}$$
\end{sol}
\section{Nulidad de una Transformación Lineal}
\begin{dfn}
Sean $V$ y $W$ dos espacios vectoriales sobre $\mathbb{K}$ y $T$ es una transformación lineal. La nulidad de $T$ se define como la dimensión de $Nu(T)$ y se denota por $v(T)$. 
\end{dfn}
\begin{theorem}[Corolario]
Al ser $Nu(T)$ un subespacio de $V$, entonces $v(T) \leq~dimV$
\end{theorem} 
\begin{ejemplo}
Determine la dimensión de $Nu(T)$, donde:
$$\svpdos{Nu(T)}{a}{b}{c}{\begin{array}{r}
        a + b = 0\\
        b + c = 0
    \end{array}}$$
\end{ejemplo}
\begin{sol}
Para determinar la dimensión de $Nu(T)$ debemos encontrar una base, para eso tomamos un vector genérico y le reemplazamos las condiciones:
\begin{align*}
    a + b = 0 &\rightarrow a = -b\\
    b + c = 0 &\rightarrow c = -b
\end{align*}
Al reemplazar, obtenemos
$$ax^2 + bx +c = (-b)x^2 + bx + (-b) = (b)(-x^2 + x - 1)$$
Entonces una base para $Nu(T)$ es:
$$B_{Nu(T)} = \llav{-x^2 + x -1}$$
Como hay un vector en la base, se puede concluir que $v(T) = 1$
\end{sol}
\section{Imagen de una Transformación Lineal}
Sean $V$ y $W$ dos espacios vectoriales y $T : V \to W$ una transformación lineal. La imagen de $T$ se define como el conjunto dado por: 
$$Im(T) = \{w \in W: \exists v \in V\ , \ T(v) = w\}$$.
\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales y $T : V \to W$ una transformación lineal. Entonces $Im(T)$es un subespacio vectorial de $W$.
\end{theorem}

\begin{proof}
Veamos que $Im(T)$ es un subespacio vectorial de $W$.
\begin{enumerate}[i.]
\item Como $T(\mathbf{0}_V)=\mathbf{0}_W$ , entonces $\mathbf{0}_W \in Im(T)$.
\item Supongamos que $u , v \in Im(T)$, entonces existen vectores $x$ e $y \in V$ tales que $T(x) = u$ y $T(y) = v$, al sumar estas dos ecuaciones, tenemos que $T(x) + T(y) = u + v$. Esto es lo mismo que $T(x + y) = (u + v)$, Como $x$ e $y$ pertenecen al espacio vectorial $V$, la suma de dichos vectores también pertenece a $V$, por lo que se cumple la definición de $Im(T)$ y $u+ v \in Im(T)$
\item Si $\alpha \in \mathbb{K}$ y $u \in Im(T)$, se tiene que
existe un vector $x \in V$, tal que $T(x) = u$, si multiplicamos $\alpha$ a ambos lados tenemos que $\alpha T(x) = \alpha u$, esto es lo mismo que $T(\alpha x) = \alpha u$, como $x \in V$, el vector $\alpha x$ también pertenece a $V$, por lo que se cumple con la definición de $Im(T)$ y $\alpha u \in Im(T)$
\end{enumerate}
Así tenemos que $Im(T)$ es un subespacio vectorial de $W$.
\end{proof}

\begin{ejemplo}
Sea $T: \pdos \to \rtres$ una transformación lineal definida por:
$$T(ax^2 + bx + c) = \vectrtrescent{a+b}{2a+b-c}{a-c}$$
Determine $Im(T)$
\end{ejemplo}

\begin{sol}
La $Im(T)$ se define como el conjunto de todos los vectores de $\rtres$ que se obtienen al transformar los vectores de $\pdos$:
$$T(ax^2 + bx + c) = \vectrtrescent{x}{y}{z}$$
Entonces
$$\vectrtrescent{a+b}{2a+b-c}{a-c} = \vectrtrescent{x}{y}{z}$$
Sabemos que dos vectores son iguales si y solo si sus componentes también lo son, entonces obtenemos el siguiente sistema de ecuaciones
$$\reducir{rrr|r}{
1&1&0&x\\
2&1&-1&y\\
1&0&-1&z
}$$
Resolviéndolo por el método de Gauss, obtenemos:
$$\reducir{rrr|r}{
1&1&0&x\\
2&1&-1&y\\
1&0&-1&z
}
\underrightarrow{\begin{array}{r}
    2f_1 - f_2\\
    f_1 - f_3
\end{array}}
\reducir{rrr|r}{
1&1&0&x\\
0&1&1&2x-y\\
0&1&1&x-z
}
\underrightarrow{f_2 - f_3}
\reducir{rrr|r}{
1&1&0&x\\
0&1&1&2x-y\\
0&0&0&x-y+z
}$$
Por lo tanto, Para que el sistema sea consistente y existan los vectores que se deben transformar, se obtiene lo siguiente
$$\svrtres{Im(T)}{x}{y}{z}{\begin{array}{r}
    x-y+z = 0
    \end{array}}$$
\end{sol}
\section{Rango de una Transformación Lineal}
\begin{dfn}
Sean $V$ y $W$ dos espacios vectoriales sobre $\mathbb{K}$ y $T$ es una transformación lineal. El $rango$ de $T$ se define como la dimensión de $Im(T)$ y se denota por $\rho(T)$.
\end{dfn}
\begin{theorem}[Corolario]
Al ser $Im(T)$ un subespacio de $W$, entonces $\rho(T) \leq~dimW$
\end{theorem} 
\begin{ejemplo}
Determine la dimensión de $Im(T)$, donde:
$$\svrtres{Im(T)}{x}{y}{z}{\begin{array}{r}
    x-y+z = 0
    \end{array}}$$
\end{ejemplo}
\begin{sol}
Para determinar la dimensión de $Im(T)$ debemos encontrar una base, para eso tomamos un vector genérico y le reemplazamos las condiciones:
\begin{align*}
    x = y-z
\end{align*}
Al reemplazar, obtenemos
$$\vectrtrescent{x}{y}{z} = \vectrtrescent{y-z}{y}{z} = y\vectrtrescent{1}{1}{0} + z\vectrtres{-1}{0}{1}$$
Entonces una base para $Im(T)$ es:
$$B_{Im(T)} = \llav{\vectrtrescent{1}{1}{0},\vectrtres{-1}{0}{1}}$$
Como hay dos vectores en la base, se puede concluir que $\rho(T) = 2$
\end{sol}

\newpage
\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales sobre $\mathbb{K}$ y $T : V \to W$ una transformación lineal. Si $B_1 = \conjvect{v}{k}$ es una base para $Nu(T)$, entonces si $B_2 = \{v_1 , \ldots , v_{k}, \ldots , v_n\}$ es una base para $V$, se tiene que $\{T(v_{k+1}), \ldots , T(v_n)\}$ es una base para $Im(T)$.
\end{theorem}

\begin{proof}

Se demostrará que el conjunto $\left\{T(v_{k+1}),\ \hdots,\ T(v_n)\right\}$ genera a $Im(T)$.\\
Tomando un vector $u$ que pertenezca a $Im(T)$, existirá un $v$ que
pertenece a $V$ tal que $T(v)$ = $u$
y como $v$ pertenece a $V$, entonces es una combinación lineal de $B_{2}$, por lo tanto:
$$v=\alpha_1v_1+\alpha_2v_2+\hdots+\alpha_kv_k+\alpha_{k+1}v_{k+1}+\hdots+\alpha_nv_n$$
Transformando a ambos lados de la igualdad y aprovechando la propiedad de linealidad, nos queda:
$$T(v)=T(\alpha_1v_1+\alpha_2v_2+\hdots+\alpha_kv_k)+\alpha_{k+1}T(v_{k+1})+\hdots+\alpha_nT(v_n)$$


Como el conjunto de vectores $\conjvect{v}{k}$ conforma una base para $Nu(T)$, entonces cualquier  transformación lineal de la combinación lineal de esos vectores, nos llevará al $0_w$, por lo que nos queda:
$$T(v)=\alpha_{k+1}T(v_{k+1})+\cdots+\alpha_nT(v_n)$$
Pero sabemos que $T(v) = u$, entonces:
$$u =\alpha_{k+1}T(v_{k+1})+\cdots+\alpha_nT(v_n)$$

Demostrando así que el conjunto $\left\{T(v_{k+1}),\ \hdots ,\ T(v_n)\right\}$ genera a $Im(T)$.\\

Ahora se demostrará que el conjunto $\left\{{T(v}_{k+1}),\ \hdots ,\ {T(v}_n)\ \right\}$ es linealmente independiente, igualando una combinación lineal de estos vectores al $0_w$.
$$\alpha_{k+1}T(v_{k+1})+\hdots+\alpha_nT(v_n) = 0_w$$
Gracias a la linealidad de una transformación lineal, podemos reescribir esa ecuación de esta manera:
$$T(\alpha_{k+1}v_{k+1}+\cdots+\alpha_nv_n) = 0_w$$
como sabemos, una combinación lineal de vectores nos devuelve otro vector, entonces:
$$x = \alpha_{k+1}v_{k+1}+\cdots+\alpha_nv_n$$
Teniendo así:
$$T(x) = 0_w$$
Por definición, sabemos que si la transformación de un vector nos lleva al neutro del espacio de llegada, entonces ese vector pertenece a $Nu(T)$, y si pertenece al núcleo, entonces se puede escribir como combinación lineal de su base.
$$ x = \alpha_1v_1 + \alpha_2v_2 + \cdots + \alpha_kv_k$$
pero sabemos que $x$ es un vector resultante de una combinación lineal, entonces: 
$$\alpha_{k+1}v_{k+1}+\cdots+\alpha_nv_n = \alpha_1v_1 + \alpha_2v_2 + \cdots + \alpha_kv_k$$
Podemos reescribir esta ecuación como:
$$-\alpha_1v_1 - \alpha_2v_2 - \cdots - \alpha_kv_k + \alpha_{k+1}v_{k+1}+\cdots+\alpha_nv_n = 0_v$$
Como es una combinación lineal de los vectores de la base de $V$ igualada al $0_v$, automáticamente los escalares son iguales a 0.
$$\alpha_1 = \alpha_2 =  \cdots = \alpha_k = \alpha_{k+1} = \cdots=\alpha_n = 0$$

Demostrando así que el conjunto $\left\{T(v_{k+1}),\ \hdots,\ T(v_n) \right\}$ es linealmente independiente.


Como el conjunto $\left\{T(v_{k+1}),\ \hdots,\ T(v_n) \right\}$ es linealmente independiente y genera a $Im(T)$, entonces conforma una base para $Im(T)$.
\end{proof}

\begin{theorem}[Teorema de las Dimensiones]
Sea $T: V \to W$ una transformación lineal, entonces $$v(T) + \rho(T) = dimV$$
\end{theorem}
\begin{proof}
Sea $V$ un $\mathbb{K}$-espacio vectorial de dimensión n, $T: V \to W$ una transformación lineal y Sea $B_{1}$ es una base para $Nu(T)$.
$$B_1 = \conjvect{v}{k}$$
Sabemos que $Nu(T)$ es un subespacio de $V$, por lo tanto $v\left(T\right)\le dimV$,
como hay k vectores en la base de $Nu(T)$, podemos llegar a obtener una base para $V$. Usando la completación de Bases.
$$B_2 =\llav{v_1,v_2, \hdots ,\ v_k,v_{k+1},\ \hdots,\ v_n}$$

Ahora, por el \textbf{teorema 9.9}, sabemos que $\left\{T(v_{k+1}),\ \hdots,\ T(v_n) \right\}$ es una base para $Im(T)$

$$ B_3 = \left\{T(v_{k+1}),\ \hdots,\ T(v_n) \right\}$$
Entonces sabemos que:
\begin{align*}
    v(T) &= k\\
    \rho(T) &= n-k
\end{align*}
ya que hay k vectores en $B_1$ y n-k vectores en $B_3$, y al sumar estas dos igualdades, obtenemos:
$$ v(T) + \rho(T) = k + n - k$$
Y por lo tanto:
$$v(T) + \rho(T) = n$$
Pero $dimV = n$, por lo tanto:
$$ v(T) + \rho(T) = dimV$$
\end{proof}

\newpage

\begin{ejemplo}
Determine la nulidad y el rango de la siguiente transformación lineal 
$$T: \rtres \to \mdosxdos$$
$$T\vectrtres{a}{b}{c} = \matrdxd{2a+b & b-c}{b+3c & a+c}$$
\end{ejemplo}
\begin{sol}
    Para este tipo de ejercicios en los que nos piden las dimensiones de los espacios asociados a una transformación lineal, normalmente empezamos por el núcleo de $T$, por lo que:
    
    $$T\vectrtres{a}{b}{c} = \matrdxd{0&0}{0&0}$$
    Entonces:
    $$\matrdxd{2a+b & b-c}{b+3c & a+c} = \matrdxd{0&0}{0&0}$$
    Lo que desemboca el siguiente sistema homogéneo:
    
    $$\reducir{rrr|r}{
        2&1&0&0\\
        0&1&-1&0\\
        0&1&3&0\\
        1&0&1&0
    } \underrightarrow{f_1 - 2f_4}
    \reducir{rrr|r}{
        2&1&0&0\\
        0&1&-1&0\\
        0&1&3&0\\   
        0&1&-2&0
    } \underrightarrow{\begin{array}{c}
        f_2 - f_3\\
        f_2 - f_4
    \end{array}}
    \reducir{rrr|r}{
        2&1&0&0\\
        0&1&-1&0\\
        0&0&-4&0\\
        0&0&1&0
    } \underrightarrow{f_3 + 4f_4}
    \reducir{rrr|r}{
        2&1&0&0\\
        0&1&-1&0\\
        0&0&-4&0\\
        0&0&0&0
    }        
    $$
    Como tenemos tres filas válidas y tres variables, el sistema tiene solución única y además es la solución trivial, por lo tanto $a = 0 , b = 0$ y $c = 0$ 
    lo que implica que $Nu(T) = \{0_v\}$ y entonces $v(T) = 0$, por el teorema de las dimensiones:
    
    \begin{align*}
        v(T) + \rho(T) &= dimV\\
        0 + \rho(T) &= 3\\
        \rho(T) &= 3
    \end{align*}
    
    Así entonces la dimensión de $Im(T)$ es de tres, finalizando el ejercicio
    
\end{sol}

\begin{theorem}
Sean $V$ y $W$ dos \dobleK-espacios vectoriales. Suponga que $V$ es un espacio de dimensión n, sea $B = \conjvect{v}{n}$ una base de $V$ y \conjvect{w}{n} vectores arbitrarios de $W$. Entonces, existe una única transformación lineal $T: V \to W$ tal que $T(v_i) = w_i$ con  $i = 1,2, \hdots, n$
\end{theorem}

\begin{theorem}[Corolario]
Sean $V$ y $W$ dos \dobleK-espacios vectoriales. Suponga que $V$ es un espacio de dimensión n, sea $B = \conjvect{v}{n}$ una base de $V$.\\
Si $T$ y $L$ son dos transformaciones lineales $V \to W$ tales que $T(v_i) = L(v_i)$ con  $i = 1,2, \hdots, n$, entonces $T = L$
\end{theorem}
\newpage
\begin{ejemplo}
Determine, de ser posible, una transformación lineal $T: \rdos \to \rtres$ tal que: $T\vectrdos{1}{0} = \vectrtres{1}{2}{3}$ y $T\vectrdos{1}{1} = \vectrtres{-4}{0}{5}$
\end{ejemplo}

\begin{sol}
    Para resolver este tipo de ejercicios, necesitamos saber si los vectores que se transforman, conforman una base para el espacio de salida, en este caso $\rdos$.\\
    
    
    Como el conjunto \llav{\vectrdos{1}{0}, \vectrdos{1}{1}} tiene la misma cantidad de vectores que la dimensión de $\rdos$, solo debemos comprobar si es linealmente independiente maximal o generador minimal para que automáticamente se convierta en una base, el camino más fácil para determinarlo, es verificar la independencia lineal. Lo cual se puede ver a simple vista ya que ambos vectores no son múltiplos, entonces ese conjunto es linealmente independiente, y por lo tanto, conforma una base para $\rdos$\\
    
    Ahora que sabemos que dicho conjunto conforma una base, entonces:
    $$\alpha_1 \vectrdos{1}{0} + \alpha_2 \vectrdos{1}{1} = \vectrdos{a}{b}$$
    Ahora necesitamos encontrar los escalares con los que es posible la combinación lineal:
    $$\reducir{rr|r}{
        1&1&a\\
        0&1&b
    }$$
    Como la matriz ya es escalonada, entonces: $\alpha_2 = b$ y $\alpha_1 = a-b$, Ahora:
    $$(a-b) \vectrdos{1}{0} + (b) \vectrdos{1}{1} = \vectrdos{a}{b}$$
    Transformando a ambos lados y aprovechando la linealidad:
    \begin{align*}
        T\vectrdos{a}{b} &= (a-b)T\vectrdos{1}{0} + (b) T\vectrdos{1}{1}\\
        T\vectrdos{a}{b}&= (a-b)\vectrtres{1}{2}{3} + (b)\vectrtres{-4}{0}{5}\\
        T\vectrdos{a}{b}&= \vectrtrescent{a-b}{2a-2b}{3a-3b} + \vectrtres{-4b}{0}{5b}\\
        T\vectrdos{a}{b}&= \vectrtrescent{a-5b}{2a-2b}{3a + 2b}
    \end{align*}

    Así entonces, se obtuvo una transformación lineal que cumple con las condiciones pedidas
    
    
    
\end{sol}

\section{Isomorfismos entre Espacios Vectoriales}
\begin{dfn}
Sea $T: V \to W$ una transformación lineal, se dice que $T$ es inyectiva (Monomorfismo) si y solo si, $\forall v, u$ y $v \neq u$, entonces $T(v) \neq T(u)$
\end{dfn}
\begin{theorem}
Sean $V,W$ dos $\mathbb{K}$-espacios vectoriales y $T: V \to W$ una transformación lineal, entonces $T$ es inyectiva si y solo si $Nu(T) = \{0_V\}$
\end{theorem}
\begin{proof}
Se debe demostrar en ambos sentidos, es decir:
\begin{enumerate}[i.]
    \item Supongamos que $T$ es inyectiva, entonces tenemos que $\forall v \in Nu(T)$, se cumple:
    $$T(v) = 0_W$$
    $$T(v) = T(0_V)$$
    Como se supuso que $T$ es inyectiva, entonces $v = 0_V$ y por lo tanto $Nu(T) = \{ 0_V\}$.
    \item Supongamos que $Nu(T) = \{ 0_V\}$ y que $\forall v,u \in V, T(v) = T(u)$, esto es lo mismo que $T(v) - T(u) = 0_W$, haciendo uso de la linealidad de la transformación, tenemos que $T(v-u) = 0_W$, por definición, el vector $v-u$ pertenece a $Nu(T)$, entonces $v-u = 0_V$ ya que el núcleo es el espacio trivial, de esta manera $v = u$, por lo tanto $T$ es inyectiva
\end{enumerate}
\end{proof}
\begin{theorem}
Sea $V$ y $W$ dos espacios vectoriales sobre $\mathbb{K}$ y $T : V \to W$ una transformación lineal inyectiva. Sea $B$ un subconjunto de $V$ linealmente independiente, entonces $T(B)$ es un subconjunto de $W$ linealmente independiente. 
\end{theorem}

\begin{proof}
Sea $B = \conjvect{v}{n}$ un conjunto linealmente independiente y $T(B) = \{T(v_1), T(v_2), \ldots , T(v_k)\}$. Si $\alpha_1 T(v_1) + \ldots + \alpha_k T(v_k) = 0$ entonces $T(\alpha_1 v_1 + \ldots +\alpha_k v_k) = \mathbf{0}_W$. Como $T$ es inyectiva se tiene que $\alpha_1 v_1 + \ldots + \alpha_k v_k = \mathbf{0}_V$ y por el hecho de ser $B$ linealmente independiente se tiene que $\alpha_i = 0$ para todo $i = 1, 2, \ldots , k$. Obtenemos entonces que $\{T(v_1), \ldots , T(v_k)\}$ es linealmente independiente.
\end{proof}

\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales sobre $\mathbb{K}$. Si $B$ es una base para $V$ y $T : V \to W$ es una transformación lineal inyectiva, entonces $T(B)$ es una base para la $Im(T)$ y si $V$ es de dimensión finita $n$ se tiene que $n = \rho(T)$.\\

\end{theorem}
\begin{dfn}
Sea $T: V \to W$ una transformación lineal, se dice que $T$ es sobreyectiva (Epimorfismo) si $Im(T) = W$
\end{dfn}
\begin{dfn}
Sean $V$ y $W$ dos espacios vectoriales. Si existe una transformación lineal $T : V \to W$ que es inyectiva y sobreyectiva, decimos que $V$ y $W$ son isomorfos y se denotan como $V \cong W$, además $T$ es un isomorfismo de espacios vectoriales.
\end{dfn}

\begin{dfn}
Sea $T: V \to W$ una transformación lineal, T es biyectiva (Isomorfismo) si y solo si T es Inyectiva y Sobreyectiva
\end{dfn}

\begin{theorem}
Sean $V$ y $W$ dos \dobleK-espacios vectoriales, donde $dimV = dimW$, y sea $T: V \to W$ una transformación lineal. $T$ es inyectiva si y solo si $T$ es sobreyectiva
\end{theorem}

\begin{theorem}
Sea $V$ un espacio vectorial de dimensión $n$ sobre $\mathbb{R}$. Entonces la transformación de coordenadas respecto a $B$ $T : V \to \rn$ es una transformación lineal.
\end{theorem}

\begin{proof}
La prueba sigue fácilmente del hecho de observar que $[v + w]_B = [v]_B + [w]_B$ y que $[\alpha v]_B = \alpha [v]_B$.
\end{proof}

\section{Transformaciones Lineales Invertibles}
\begin{dfn}
Una transformación $T: V \to W$ se dice invertible si existe otra transformación $L: W \to V$ tal que $L \circ T = I_V$ y $T \circ L = I_W$, donde $I_V , I_W$ son las transformaciones identidad en $V$ y $W$ respectivamente 
\end{dfn}
\begin{obs}
\begin{itemize}
    \item El hecho de que exista $L: W \to V$ tal que $L \circ T = I_V$, no garantiza que $T \circ L = I_W$ ya que la composición es una operación no conmutativa.
    \item Si $T$ es una transformación lineal invertible, $L$ es una transformación única y se denota como $T^{-1}$
\end{itemize}
\end{obs}

\begin{theorem}
Sean $V,W$ dos $\dobleK$-espacios vectoriales, $T: V \to W$ una transformación lineal. $T$ es invertible si y solo si $T$ es Biyectiva.\\
\end{theorem}



\begin{ejemplo}
Determine de ser posible, si $T: \pdos \to \rtres$ es invertible cuando:
$$T(ax^2 + bx + c) = \vectrtres{a+2c}{3a-b}{b+c}$$
\end{ejemplo}

\begin{sol}
    Para determinar si $T$ es invertible, veamos primero si es biyectiva, como $dim\pdos = dim\rtres$, solo basta con probar si es inyectiva o sobreyectiva, lo más sencillo es probar la inyectividad, entonces:
    $$T(ax^2 + bx + c) = \vectrtres{a+2c}{3a-b}{b+c} = \vectrtres{0}{0}{0}$$
    $$\reducir{rrr|r}{
        1&0&2&0\\
        3&-1&0&0\\
        0&1&1&0
    }\underrightarrow{3f_1 - f_2}
    \reducir{rrr|r}{
        1&0&2&0\\
        0&1&6&0\\
        0&1&1&0
    }\underrightarrow{f_2 - f_3}
    \reducir{rrr|r}{
        1&0&2&0\\
        0&1&6&0\\
        0&0&5&0
    }$$
    Como el sistema nos quedó consistente con solución única, entonces $Nu(T) = \{0_V\}$, por lo tanto $T$ es inyectiva, y biyectiva, por lo que también es invertible
\end{sol}

\begin{comment}
\begin{proof}
Se debe demostrar en ambos sentidos:
\begin{enumerate}[i.]
    \item Suponga que $T$ es invertible. Esto significa que existe una función $T^{-1}: W \to V$ tal que $T^{-1} \circ T = I_V$ y $T \circ T^{-1} = I_W$.\\
    Ahora, si $v \in Nu(T)$, entonces:
    \begin{align*}
        T(v) &= 0_W\\
        T^{-1}(T(v)) &= T^{-1}(0_W)\\
        v &= T^{-1}(0_W) = 0_V
    \end{align*}
    Esto significa que $Nu(T) = \llav{0_V}$, por lo tanto $T$ es inyectiva gracias al \textbf{Teorema 9.11}.\\
    Ahora, si $w \in W$, se tiene que $v = T^{-1}(w)\in V$, entonces:
    \begin{align*}
        v &= T^{-1}(w)\\
        T(v) &= T(T^{-1}(w))\\
        T(v) &= w
    \end{align*}
    Esto significa que para cualquier $w \in W$, existe un $v \in V$ tal que $T(v) = w$, por lo tanto $Im(T) = W$ y T es sobreyectiva
    
    \item Ahora supongamos que $T$ es biyectiva y que existe $L: W \to V$ tal que $L(w) = v$.\\
\end{enumerate}
\end{proof}
\end{comment}





\newpage
\section{Operaciones con Transformaciones Lineales}
\begin{dfn}
Sean $V$ y $W$ dos espacios vectoriales sobre un campo $\mathbb{K}$ y $T$ y $L$ dos transformaciones lineales de $V$ en $W$ y $\lambda \in \mathbb{K}$. Entonces 
\begin{enumerate}
\item $T + L : V \to W \qquad (T + L)(v) = T(v) + L(v)$
\item $\lambda T : V \to W \qquad (\lambda T)(v) = \lambda T(v)$
\end{enumerate}
Son transformaciones lineales.
\end{dfn}

\begin{proof}
\begin{align*}
\bullet (T+L)(u+v) &=T(u+v) + L(u+v)\\
&= T(u) + T(v) + L(u) + L(v)\\
&= (T+L)(u) + (T+L)(v)\\
\\
\bullet(T+L)(\alpha u) &= T(\lambda u) + L(\lambda u)\\
&= \lambda T(u)+ \lambda L(u)\\
&= \lambda (T+L)(u) \\
\\
\bullet \lambda T(u+v) &= \lambda [T(u+v)]\\
&= \lambda [T(u) + T(v)]\\
&= (\lambda T)(u) + (\lambda T)(v)\\
\\
\bullet (\lambda T)(\beta u) &= \lambda T(\beta u)\\
&=\lambda \beta T(u)\\
&= \beta (\lambda T)(u)
\end{align*}
\end{proof}

\begin{theorem}
Sean $V$ y $W$ espacios vectoriales y sea $T(V, W) = \{T : V \to~W$ ; T es una transformación lineal\}. Entonces $T(V, W)$ con las operaciones definidas anteriormente es un espacio vectorial.
\end{theorem}

\begin{proof}
Se utilizará el teorema de caracterización del subespacio para esta prueba.
\begin{enumerate}[i.]
    \item Si definimos una transformación lineal $T: V \to W$ tal que $T(v) = 0_W$, es decir, la transformación nula, ésta actúa como el elemento neutro de la adición del espacio $T(V,W)$ ya que para $L: V \to W$, $(T + L)(v) = T(v) + L(v) = 0_W + L(v) = L(v)$, entonces $T(V,W)$ es no vacío.
    \item En la prueba anterior se demostró que $(T+L)(v)$ también es una transformación lineal.
    \item En la prueba anterior se demostró que $\lambda T(v)$ también es una transformación lineal.
\end{enumerate}
Por lo que el conjunto de todas las transformaciones lineales $T(V,W)$ es un espacio vectorial.
\end{proof}

\begin{theorem}
Sean $V,U,W$ tres espacios vectoriales sobre un campo $\mathbb{K}$. Sean $T: V \to U$ y $L: U \to W$ dos transformaciones lineales, entonces la composición de $T$ y $L$ se define como:
\begin{align*}
    L \circ T \ : \ V \to W & \\
    (L \circ T)(v) = L(T(v)) &; \forall v \in V
\end{align*}
Y también es una transformación lineal
\end{theorem}
\begin{proof}
Para demostrar la linealidad de esta transformación, tenemos que probar:
\begin{enumerate}[i.]
    \item $(L \circ T)(v + u) = L(T(v+u))$ como $T$ es una transformación lineal, entonces es igual a $L(T(v) + T(u)) = L(T(v)) + L(T(u)) = (L \circ T)(v) + (L \circ T)(u)$
    \item $(L \circ T)(\alpha v) = L(T(\alpha v))$ como $T$ es una transformación lineal, entonces es igual a $L(\alpha T(v)) = \alpha L(T(v)) = \alpha (L\circ T)(v)$
\end{enumerate}
Demostrando así que $(L \circ T)$ es una transformación lineal
\end{proof}

\begin{obs}
Sea $T: V \to V$ un operador lineal, y la composición $(T \circ T) : V \to V$, se denota de la siguiente manera $(T \circ T)(v) = T(T(v)) = 
T^2(v)$
\end{obs}
\section{Representación Matricial de una Transformación Lineal}

\begin{dfn}
Sean $V$ y $W$ dos \dobleK-espacios vectoriales de dimensión finita.
con $dimV = n$ y $dimW = m$. Sean $B_1 = \conjvect{v}{n}$ una base de $V$ y $B_2 = \conjvect{w}{m}$ una base de $W$. Sea $T : V \to W$ una
transformación lineal y $A$ una matriz de orden $m\times n$\\
Se dice que $A$ es la matriz de representación de una transformación lineal si cumple que:
$$\forall v \in V \ ; \ A \cdot [v]_{B_1} = [T(v)]_{B_2}$$
y la matriz $A$ se denota como $[T]_{B_1 B_2}$, por lo que la igualdad anterior nos queda como:
$$\forall v \in V \ ; \ [T]_{B_1 B_2} \cdot [v]_{B_1} = [T(v)]_{B_2}$$
\end{dfn}

\begin{theorem}
Sean $V$ y $W$ dos \dobleK-espacios vectoriales de dimensión n y m, $B_1=\conjvect{v}{n}$  y $B_2=\conjvect{u}{m}$  dos bases ordenadas de $V$ y $W$ respectivamente, y $[T]_{B_1 B_2}$ es la matriz de representación de una transformación lineal entonces:
\[[T]_{B_1 B_2}= \begin{pmatrix}
\vdots & \vdots & \vdots & \vdots\\
\left[T(v_1)\right]_{B_2}&\left[T(v_2)\right]_{B_2}& \hdots & \left[T(v_n)\right]_{B_2}\\
\vdots & \vdots & \vdots & \vdots\\
\end{pmatrix}\]
\end{theorem}

\newpage

\begin{theorem}
Sean $V$ y $W$ dos \dobleK-espacios vectoriales y $S, T : V \to W$ transformaciones lineales arbitrarias y $\alpha \in \dobleK$. Considere $B_1$ y $B_2$ bases ordenadas de $V$ y $W$ respectivamente, entonces:
\begin{enumerate}
    \item $[T+S]_{B_1 B_2} = [T]_{B_1 B_2} + [S]_{B_1 B_2}$
    \item $[\alpha T]_{B_1 B_2} = \alpha [T]_{B_1 B_2}$
\end{enumerate}
\end{theorem}

\begin{theorem}
Sean $V,W$ y $U$ tres \dobleK-espacios vectoriales, $T: V \to W, S: W \to Z$ transformaciones lineales. Si $B_1, B_2$ y $B_3$ son bases de $V,W$ y $Z$ respectivamente, entonces $[S \circ T]_{B_1 B_3} = [S]_{B_2 B_3} [T]_{B_1 B_2}$
\end{theorem}
\begin{proof}
Para cada $v \in V$ se tiene que:
\begin{align*}
    ([S]_{B_2 B_3} [T]_{B_1 B_2})[v]_{B_1} &= [S]_{B_2 B_3}([T]_{B_1 B_2}[v]_{B_1})\\
    &= [S]_{B_2 B_3} [T(v)]_{B_2}\\
    &= [S(T(v))]_{B_3}\\
    &= [(S\circ T)(v)]_{B_3}
\end{align*}
Como esto es válido para todo $v \in V$, entonces $[S]_{B_2 B_3}[T]_{B_1 B_2} = [S \circ T]_{B_1 B_3}$
\end{proof}

\begin{theorem}
Sean $V,W$ dos \dobleK-espacios vectoriales, $T: V \to W$ una transformación lineal, suponga que $dimV = n = dimW, B_1$ y $B_2$ son bases de $V$ y $W$ respectivamente. $T$ es invertible si y solo si $[T]_{B_1 B_2}$ lo es. de otra forma:
$$[T]^{-1}_{B_1 B_2} = [T^{-1}]_{B_2 B_1}$$
\end{theorem}
\begin{proof}
Se debe demostrar en ambos sentidos, por lo que:
\begin{enumerate}[i.]
    \item Suponga que $T$ es invertible, entonces por el \textbf{Teorema 9.20}, se tiene que:
    $$[T]_{B_1 B_2} [T^{-1}]_{B_2 B_1} = [T \circ T^{-1}]_{B_2 B_2} = [I_W]_{B_2 B_2}$$
    De la misma manera:
    $$[T^{-1}]_{B_2 B_1} [T]_{B_1 B_2}  = [T^{-1} \circ T]_{B_1 B_1} = [I_V]_{B_1 B_1}$$
    Por otra parte $[I_W]_{B_2 B_2} = I_n = [I_V]_{B_1 B_1}$
    Por lo tanto, $[T]_{B_1 B_2}$ es invertible y $[T]^{-1}_{B_1 B_2} = [T^{-1}]_{B_2 B_1}$
    \item Suponga que $[T]_{B_1 B_2}$ es invertible. Si $v \in Nu(T)$ entonces:
    \begin{align*}
        [T(v)]_{B_2} &= [T]_{B_1 B_2}[v]_{B_1}\\
        [0_W]_{B_2} &= [T]_{B_1 B_2}[v]_{B_1}\\
        0_{\dobleK^n} &= [v]_{B_1} 
    \end{align*}
    Esto implica que $v = 0_V$, entonces $Nu(T) = \{0_V\}$, por lo que $T$ es inyectiva.\\
    Así $v(T) = 0$ y por el teorema de las dimensiones, $\rho(T) = dimV = n = dimW$, como $Im(T) \subseteq W$ y $\rho(T) = dimW$, implica que $Im(T) = W$ por lo que $T$ también es sobreyectiva, por lo tanto también es biyectiva e invertible.
\end{enumerate}
\end{proof}

\begin{ejemplo}
Sea $T: \pdos \to \rdos$ una transformación lineal definida por:
$$T(ax^2 + bx + c) = \vectrdos{2a-3b+c}{a+2b-c}$$
y sean $B_1 = \llav{x^2 +1, x^2 + x +4, x-4}$ y $B_2 = \llav{\vectrdos{2}{1},\vectrdos{3}{1}}$ dos bases ordenadas, determine la matriz de representación de $T$
\end{ejemplo}

\begin{sol}
    Para determinar la matriz de representación, debemos transformar los vectores de $B_1$ y obtener sus coordenadas con respecto a $B_2$, así entonces:
    \begin{align*}
        T(x^2 + 1) = \vectrdos{3}{0} && T(x^2 + x +4) = \vectrdos{3}{-1} && T(x-4) = \vectrdos{-7}{6}
    \end{align*}
    Ahora, para encontrar las coordenadas igualamos los vectores a una combinación lineal de $B_2$:
    \begin{align*}
        T(x^2 + 1) = \vectrdos{3}{0} = \alpha_1 \vectrdos{2}{1} + \alpha_2 \vectrdos{3}{1}\\
        T(x^2 + x +4) = \vectrdos{3}{-1} = \beta_1 \vectrdos{2}{1} + \beta_2 \vectrdos{3}{1}\\
        T(x-4) = \vectrdos{-7}{6} = \gamma_1 \vectrdos{2}{1} + \gamma_2 \vectrdos{3}{1}
    \end{align*}
    Para ahorrar tiempo, pondremos el sistema aumentado tres veces, representando las tres combinaciones lineales anteriores:
    $$\reducir{rr|r|r|r}{
        2&3&3&3&-7\\
        1&1&0&-1&6
    }\underrightarrow{f_1 - 2f_2}
    \reducir{rr|r|r|r}{
        2&3&3&3&-7\\
        0&1&3&5&-19
    }$$
    así entonces:
    \begin{align*}
        \alpha_1 = -3 \ ; \ \alpha_2 = 3 &&
        \beta_1 =-6 \ ; \ \beta_2 = 5 &&
        \gamma_1 = 25 \ ; \ \gamma_2 = -19
    \end{align*}
    por lo tanto:
    \begin{align*}
        [T(x^2 + 1)]_{B_2} = \vectrdos{-3}{3} && [T(x^2 + x +4)]_{B_2} = \vectrdos{-6}{5} && [T(x-4)]_{B_2} = \vectrdos{25}{-19}
    \end{align*}
    Las cuales son las columnas de la representación matricial:
    $$[T]_{B_1 B_2} = \reducir{rrr}{
        -3&-6&25\\
        3&5&-19
    }$$
\end{sol}



